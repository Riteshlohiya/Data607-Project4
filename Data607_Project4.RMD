---
title: "Data607 Project 4"
author: "Ritesh Lohiya"
date: "April 13, 2018"
output: html_document
---

Task:

It can be useful to be able to classify new "test" documents using already classified "training" documents. A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). One example corpus: https://spamassassin.apache.org/publiccorpus/

The data is present at : http://spamassassin.apache.org/old/publiccorpus/

Libraries:

#install.packages("wordcloud")
#install.packages("tidytext")
#install.packages("tm",dependencies=TRUE)
#install.packages("RTextTools", type="source")
#install.packages("kableExtra")

```{r}
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(wordcloud)
library(tidytext)
library(tm)
library(RTextTools)
library(knitr)
library(tidyverse)
library(kableExtra)
```

#Getting the data:

```{r}
ham_file <- "C:/Users/Arnav/Desktop/CUNY/Data607/Data607_Project4/easy_ham/easy_ham/"
ham_names <- list.files(ham_file)
head(ham_names)
spam_file <- "C:/Users/Arnav/Desktop/CUNY/Data607/Data607_Project4/spam_2/spam_2/"
spam_names <- list.files(spam_file)
head(spam_names)
```

#Creating a corpus:

```{r}
ham <- VCorpus(DirSource("C:/Users/Arnav/Desktop/CUNY/Data607/Data607_Project4/easy_ham/easy_ham/"))
spam <- VCorpus(DirSource("C:/Users/Arnav/Desktop/CUNY/Data607/Data607_Project4/spam_2/spam_2/"))
```


#Adding the tags:

```{r}
meta(spam, tag = "type") <- "spam"
meta(ham, tag = "type") <- "ham"
data1 <- c(spam, ham)

```

#Cleaning the data as we do not require spaces, punctuation and numbers for out model.

```{r}
data1 <- tm_map(data1, content_transformer(function(x) iconv(x, "UTF-8", sub="byte")))
data1 <- tm_map(data1, content_transformer(tolower))
data1 <- tm_map(data1, removeNumbers)
data1 <- tm_map(data1, removeWords, stopwords("english"))
data1 <- tm_map(data1, removePunctuation)
data1 <- tm_map(data1, stripWhitespace)
```

#Wordcloud:

```{r}
wordcloud(data1, min.freq=1000)
```

#Arranging the text in matrix form where the rows represent individual terms and columns contain the texts:

```{r}
dtm <- DocumentTermMatrix(data1)
dtm
```

#Removing sparse words:

```{r}
dtm <- removeSparseTerms(dtm, 1-(10/length(data1)))
dtm
```

#Word counts:

```{r}
dtm2 <- as.matrix(dtm)
word_count <- colSums(dtm2)
word_count <- sort(word_count, decreasing=T)
words <- head(word_count, 15)
words
```

#Number of ham and spam emails:

```{r}
data2 <- as.vector(unlist(meta(data1)))
data3 <- data.frame(type = unlist(data2))
table(data3)
```

#Using create_container() function from RTextTools we will be splitting the data into train and test:

```{r}
container <- create_container(dtm,labels = data2,trainSize = 1:2728,testSize = 2729:3898,virgin = FALSE)
slotNames(container)
```

#We can use the train_model() function for training our data:

```{r}
models <- train_models(container, algorithms=c("MAXENT","SVM", "RF", "TREE"))
results <- classify_models(container, models)
head(results)
```

#Adding the correct email type to the data:

```{r}
out_data <- data.frame(
correct_type = data2[2729:3898],
maxent_type = as.character(results$MAXENTROPY_LABEL),
svm_type = as.character(results$SVM_LABEL),
rf_type = as.character(results$FORESTS_LABEL),
tree_type = as.character(results$TREE_LABEL),
stringsAsFactors = F)
out_data
```

#Maximum entropy performance:

```{r}
maxent_table <- prop.table(table(out_data$correct_type == out_data$maxent_type))
maxent_table
```

#SVM performance:

```{r}
svm_table <- prop.table(table(out_data$correct_type == out_data$svm_type))
svm_table
```
 
#Random Forest performance:

```{r}
rf_table <- prop.table(table(out_data$correct_type == out_data$rf_type))
rf_table
```

#Decision Tree performance:

```{r}
tree_table <- prop.table(table(out_data$correct_type == out_data$tree_type))
tree_table
```

#Conclusions:

we can see that the Maximum Entropy is best classifier followed by SVM and Random Forest for this model. Decision Tree is the worst classifier for this model.








